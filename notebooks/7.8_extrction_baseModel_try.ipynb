{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import transformers\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 31461120 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.81s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"lmsys/vicuna-7b-v1.3\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 39, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 11 tokens in 17.61 seconds.\n",
      "Tokens per second: 0.62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"Given the following text, extract all named entities (e.g., people, organizations, locations): 'Apple Inc. was founded by Steve Jobs in Cupertino, California.'\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") \n",
    "\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "\n",
    "torch.cuda.synchronize()  \n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)\n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 38, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 tokens in 1.58 seconds.\n",
      "Tokens per second: 0.63\n"
     ]
    }
   ],
   "source": [
    "prompt = \"From the paragraph below, identify the key phrases that summarize its main ideas: 'Climate change is affecting weather patterns worldwide, leading to more extreme weather events.'\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 50, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 50, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 tokens in 1.59 seconds.\n",
      "Tokens per second: 0.63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \" Passage:Sarah ordered a large pizza and ate only two slices. She placed the rest in the fridge to have later. Question: Why did Sarah put the leftover pizza in the fridge\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 36, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 tokens in 1.64 seconds.\n",
      "Tokens per second: 0.61\n"
     ]
    }
   ],
   "source": [
    "prompt=\"Statement:The roads are wet, and people are carrying umbrellas.What can you infer from this statement?\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 35, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 15 tokens in 24.32 seconds.\n",
      "Tokens per second: 0.62\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Complete the analogy:'Book is to Author as Painting is to________.'\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 42, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7 tokens in 10.22 seconds.\n",
      "Tokens per second: 0.68\n"
     ]
    }
   ],
   "source": [
    "prompt=\"Analyze the following customer review and extract the sentiment (positive, negative, neutral): 'The new phone is fantastic! It has a great camera, but the battery life could be better.'\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
