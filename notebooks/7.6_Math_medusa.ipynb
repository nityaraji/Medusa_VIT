{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from medusa.model.modeling_llama_kv import LlamaForCausalLM as KVLlamaForCausalLM\n",
    "from medusa.model.medusa_model import MedusaModel, MedusaConfig\n",
    "from medusa.model.kv_cache import *\n",
    "from medusa.model.utils import *\n",
    "from medusa.model.medusa_choices import *\n",
    "import transformers\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timed(wall_times, key):\n",
    "    start = time.time()\n",
    "    torch.cuda.synchronize()\n",
    "    yield\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    wall_times[key].append(elapsed_time)\n",
    "\n",
    "def medusa_forward(input_ids, model, tokenizer, medusa_choices, temperature, posterior_threshold, posterior_alpha, max_steps = 512):\n",
    "    wall_times = {'medusa': [], 'tree': [], 'posterior': [], 'update': [], 'init': []}\n",
    "    \n",
    "    with timed(wall_times, 'init'):\n",
    "        if hasattr(model, \"medusa_choices\") and model.medusa_choices == medusa_choices:\n",
    "            medusa_buffers = model.medusa_buffers\n",
    "        else:\n",
    "            medusa_buffers = generate_medusa_buffers(\n",
    "                medusa_choices, device=model.base_model.device\n",
    "            )\n",
    "        model.medusa_buffers = medusa_buffers\n",
    "        model.medusa_choices = medusa_choices\n",
    "\n",
    "        if hasattr(model, \"past_key_values\"):\n",
    "            past_key_values = model.past_key_values\n",
    "            past_key_values_data = model.past_key_values_data\n",
    "            current_length_data = model.current_length_data\n",
    "            current_length_data.zero_()\n",
    "        else:\n",
    "            (\n",
    "                past_key_values,\n",
    "                past_key_values_data,\n",
    "                current_length_data,\n",
    "            ) = initialize_past_key_values(model.base_model)\n",
    "            model.past_key_values = past_key_values\n",
    "            model.past_key_values_data = past_key_values_data\n",
    "            model.current_length_data = current_length_data\n",
    "\n",
    "        input_len = input_ids.shape[1]\n",
    "        reset_medusa_mode(model)\n",
    "        medusa_logits, logits = initialize_medusa(\n",
    "                input_ids, model, medusa_buffers[\"medusa_attn_mask\"], past_key_values\n",
    "        )\n",
    "    new_token = 0\n",
    "\n",
    "    for idx in range(max_steps): \n",
    "        with timed(wall_times, 'medusa'):\n",
    "            candidates, tree_candidates = generate_candidates(\n",
    "                    medusa_logits,\n",
    "                    logits,\n",
    "                    medusa_buffers[\"tree_indices\"],\n",
    "                    medusa_buffers[\"retrieve_indices\"],\n",
    "                )\n",
    "\n",
    "        with timed(wall_times, 'tree'):\n",
    "            medusa_logits, logits, outputs = tree_decoding(\n",
    "                    model,\n",
    "                    tree_candidates,\n",
    "                    past_key_values,\n",
    "                    medusa_buffers[\"medusa_position_ids\"],\n",
    "                    input_ids,\n",
    "                    medusa_buffers[\"retrieve_indices\"],\n",
    "                )\n",
    "\n",
    "        with timed(wall_times, 'posterior'):\n",
    "            best_candidate, accept_length = evaluate_posterior(\n",
    "                    logits, candidates, temperature, posterior_threshold, posterior_alpha\n",
    "                )\n",
    "        \n",
    "        with timed(wall_times, 'update'):\n",
    "            input_ids, logits, medusa_logits, new_token = update_inference_inputs(\n",
    "                    input_ids,\n",
    "                    candidates,\n",
    "                    best_candidate,\n",
    "                    accept_length,\n",
    "                    medusa_buffers[\"retrieve_indices\"],\n",
    "                    outputs,\n",
    "                    logits,\n",
    "                    medusa_logits,\n",
    "                    new_token,\n",
    "                    past_key_values_data,\n",
    "                    current_length_data,\n",
    "                )\n",
    "\n",
    "        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():\n",
    "            break\n",
    "\n",
    "    return input_ids, new_token, idx, wall_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.53s/it]\n",
      "Some weights of MedusaModelLlama were not initialized from the model checkpoint at lmsys/vicuna-7b-v1.3 and are newly initialized: ['medusa_head.2.0.linear.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.1.1.weight', 'medusa_head.3.1.weight', 'medusa_head.4.0.linear.weight', 'medusa_head.0.0.linear.bias', 'medusa_head.4.1.weight', 'medusa_head.3.0.linear.weight', 'medusa_head.1.0.linear.weight', 'medusa_head.0.1.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.4.0.linear.bias', 'medusa_head.0.0.linear.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.2.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "C:\\Users\\admin\\Downloads\\Medusa-main\\medusa\\model\\medusa_model.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  medusa_head_state_dict = torch.load(filename, map_location=model.device)\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 0.0.linear.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 0.0.linear.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 0.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 1.0.linear.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 1.0.linear.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 1.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 2.0.linear.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 2.0.linear.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 2.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 3.0.linear.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 3.0.linear.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 3.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 4.0.linear.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 4.0.linear.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 4.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"
     ]
    }
   ],
   "source": [
    "model_name = 'FasterDecoding/medusa-vicuna-7b-v1.3'\n",
    "\n",
    "config = MedusaConfig.from_pretrained(\n",
    "    model_name,\n",
    "    medusa_num_heads=4,\n",
    "    medusa_num_layers=1,\n",
    ")\n",
    "\n",
    "model = MedusaModel.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "medusa_choices = mc_sim_7b_63\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.\n",
    "posterior_threshold = 0.09\n",
    "posterior_alpha = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the concept of infinity and how it is used in calculus\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 127\n",
      "Compression ratio: tensor(1.0079, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Infinity is a mathematical concept that refers to a value that is greater than any finite number. It is often represented by the symbol ∞. In calculus, infinity is used to represent the limit of a function as the input value approaches a certain value. For example, the limit of a function as x approaches infinity is infinity. This is because the function approaches a value that is greater than any finite number as the input value approaches infinity. Infinity is also used to represent the slope of a line at a point where the input value is infinity. This is known as the vertical asymptote of the function.</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.858\n",
      "Wall time medusa:                            0.114\n",
      "Wall time Tree:                             77.742\n",
      "Wall time Posterior:                         0.060\n",
      "Wall time Update:                            0.121\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.985\n",
      "Wall time portion Posterior:                 0.001\n",
      "Wall time portion Update:                    0.002\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.610\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"Discuss the role of geometry in architecture and how it influences building design.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 512\n",
      "Compression ratio: tensor(1.0020, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Describe the relationship between the human body and architecture.\n",
      "Explain the concept of sustainable architecture and its importance in the design of buildings.\n",
      "Discuss the role of technology in architecture and how it has changed the profession.\n",
      "Describe the impact of cultural and social factors on building design.\n",
      "Explain the concept of adaptive reuse and its importance in preserving historic buildings.\n",
      "Discuss the role of urban planning in shaping cities and communities.\n",
      "Describe the impact of globalization on architecture and the profession.\n",
      "Explain the concept of biophilic design and its importance in creating healthy and sustainable buildings.\n",
      "Discuss the role of architectural history in shaping the profession and influencing building design.\n",
      "Explain the concept of parametric architecture and its impact on building design.\n",
      "Describe the impact of climate change on building design and sustainability.\n",
      "Discuss the role of community engagement in the design of buildings and public spaces.\n",
      "Explain the concept of inclusive design and its importance in creating accessible and equitable buildings.\n",
      "Describe the impact of natural disasters on building design and construction.\n",
      "Discuss the role of architectural education in shaping the profession and influencing building design.\n",
      "Explain the concept of human-centered design and its importance in creating buildings that meet the needs of their users.\n",
      "Describe the impact of cultural diversity on building design and the profession.\n",
      "Explain the concept of resilient design and its importance in creating buildings that can withstand natural disasters and other challenges.\n",
      "Discuss the role of architectural preservation in maintaining cultural heritage and historic buildings.\n",
      "Explain the concept of biomimicry and its impact on building design and sustainability.\n",
      "Describe the impact of global health crises on building design and construction.\n",
      "Discuss the role of architectural criticism in shaping the profession and influencing building design.\n",
      "Explain the concept of speculative design and its importance in exploring new possibilities for building design.\n",
      "Describe the impact of technological advancements on building design and construction.\n",
      "Discuss the role of architectural competitions in shaping the profession and influencing building design.\n",
      "Explain the concept of adaptive re-use and its importance in preserving historic buildings.\n",
      "Describe the impact of urbanization on building design and the environment.\n",
      "Discuss the role of architectural research in sh\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.867\n",
      "Wall time medusa:                            0.151\n",
      "Wall time Tree:                            315.584\n",
      "Wall time Posterior:                         0.274\n",
      "Wall time Update:                            0.308\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.000\n",
      "Wall time portion Tree:                      0.995\n",
      "Wall time portion Posterior:                 0.001\n",
      "Wall time portion Update:                    0.001\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.614\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Analyze the Fibonacci sequence and its occurrences in nature, such as in flower petals and shells.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 512\n",
      "Compression ratio: tensor(1.0020, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explore the golden ratio and its relationship to the Fibonacci sequence.\n",
      "Investigate the Fibonacci sequence in art and architecture, such as in the design of the Parthenon.\n",
      "Examine the Fibonacci sequence in finance and investing, such as in the stock market and real estate.\n",
      "Discuss the Fibonacci sequence in music and rhythm, such as in the harmonics of sound waves.\n",
      "Explore the Fibonacci sequence in computer science and programming, such as in algorithms and data structures.\n",
      "Investigate the Fibonacci sequence in sports and games, such as in the game of chess and the movement of athletes.\n",
      "Examine the Fibonacci sequence in language and linguistics, such as in the structure of words and sentences.\n",
      "Discuss the Fibonacci sequence in philosophy and spirituality, such as in the concept of balance and harmony.\n",
      "Explore the Fibonacci sequence in education and learning, such as in the development of problem-solving skills.\n",
      "Investigate the Fibonacci sequence in technology and innovation, such as in the development of new materials and products.\n",
      "Examine the Fibonacci sequence in social sciences and humanities, such as in the study of human behavior and culture.\n",
      "Discuss the Fibonacci sequence in environmental science and sustainability, such as in the growth patterns of plants and animals.\n",
      "Explore the Fibonacci sequence in medicine and health, such as in the growth patterns of cells and organs.\n",
      "Investigate the Fibonacci sequence in psychology and neuroscience, such as in the study of perception and cognition.\n",
      "Examine the Fibonacci sequence in business and economics, such as in the analysis of market trends and consumer behavior.\n",
      "Discuss the Fibonacci sequence in international relations and diplomacy, such as in the study of global patterns and trends.\n",
      "Explore the Fibonacci sequence in law and justice, such as in the study of legal systems and punishment.\n",
      "Investigate the Fibonacci sequence in politics and government, such as in the study of political systems and leadership.\n",
      "Examine the Fibonacci sequence in sociology and anthropology, such as in the study of social structures and cultural practices.\n",
      "Dis\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.813\n",
      "Wall time medusa:                            0.282\n",
      "Wall time Tree:                            313.618\n",
      "Wall time Posterior:                         0.158\n",
      "Wall time Update:                            0.386\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.995\n",
      "Wall time portion Posterior:                 0.001\n",
      "Wall time portion Update:                    0.001\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.624\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"If a car travels 60 miles in 1.5 hours, what is its speed in miles per hour?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 1\n",
      "Compression ratio: tensor(inf, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.808\n",
      "Wall time medusa:                            0.000\n",
      "Wall time Tree:                              0.657\n",
      "Wall time Posterior:                         0.000\n",
      "Wall time Update:                            0.000\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.000\n",
      "Wall time portion Tree:                      0.448\n",
      "Wall time portion Posterior:                 0.000\n",
      "Wall time portion Update:                    0.000\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               0.683\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"If a car travels 60 miles in 1.5 hours, what is its speed in miles per hour?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 1\n",
      "Compression ratio: tensor(inf, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remote work is a type of work that is done outside of a traditional office setting. This can include working from home, a coffee shop, or any other location that is not a traditional office. On the other hand, office-based work is done in a traditional office setting, typically during regular business hours.\n",
      "One of the main advantages of remote work is the flexibility it offers. With remote work, employees can set their own schedules and work from anywhere, as long as they have a reliable internet connection. This can be especially beneficial for parents who need to balance work and family responsibilities, or for people who have mobility issues that make it difficult to commute to an office.\n",
      "Office-based work, on the other hand, typically requires a more structured schedule and a physical presence in the office. This can be beneficial for employees who need to collaborate with colleagues in person, or who need to be available to answer phone calls or emails during regular business hours.\n",
      "Another advantage of remote work is the potential cost savings for employers. By allowing employees to work from home, companies can reduce their overhead costs for office space, equipment, and supplies. This can be especially beneficial for small businesses that are looking to cut costs.\n",
      "Office-based work, on the other hand, may require more investment in equipment and supplies, as well as the cost of renting or leasing office space. However, this can also provide a more professional and collaborative environment for employees.\n",
      "Overall, both remote work and office-based work have their own advantages and disadvantages. The choice between the two will depend on the specific needs and preferences of the employer and employees.</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.832\n",
      "Wall time medusa:                            0.112\n",
      "Wall time Tree:                            146.780\n",
      "Wall time Posterior:                         0.064\n",
      "Wall time Update:                            0.220\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.992\n",
      "Wall time portion Posterior:                 0.000\n",
      "Wall time portion Update:                    0.001\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.588\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"Find the value of x  in the equation  3x + 4 = 19 .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 286\n",
      "Compression ratio: tensor(1.0035, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(a) 3x + 4 = 19\n",
      "(b) 3x + 4 = 19\n",
      "(c) 3x + 4 = 19\n",
      "(d) 3x + 4 = 19\n",
      "\n",
      "The correct answer is (d) 3x + 4 = 19.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The given equation is 3x + 4 = 19.\n",
      "\n",
      "To solve for x, we need to isolate x on one side of the equation by performing the following steps:\n",
      "\n",
      "1. Multiply both sides of the equation by the coefficient of x (3) to get a 3x on one side and 34 on the other side:\n",
      "3(3x + 4) = 3(19)\n",
      "\n",
      "1. Add 34 to both sides of the equation to get:\n",
      "6x + 4 = 53\n",
      "\n",
      "1. Divide both sides of the equation by 6 to get the final answer in the form of x:\n",
      "x = 9\n",
      "\n",
      "Therefore, the value of x in the equation 3x + 4 = 19 is 9.\n",
      "\n",
      "Hence, the correct option is (d) 3x + 4 = 19.</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.893\n",
      "Wall time medusa:                            0.154\n",
      "Wall time Tree:                            177.475\n",
      "Wall time Posterior:                         0.056\n",
      "Wall time Update:                            0.077\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.993\n",
      "Wall time portion Posterior:                 0.000\n",
      "Wall time portion Update:                    0.000\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.601\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the perimeter of a triangle ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 1\n",
      "Compression ratio: tensor(inf, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.971\n",
      "Wall time medusa:                            0.000\n",
      "Wall time Tree:                              0.610\n",
      "Wall time Posterior:                         0.000\n",
      "Wall time Update:                            0.000\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.000\n",
      "Wall time portion Tree:                      0.386\n",
      "Wall time portion Posterior:                 0.000\n",
      "Wall time portion Update:                    0.000\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               0.633\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
