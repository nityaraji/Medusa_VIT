{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import transformers\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 31461120 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.16s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"lmsys/vicuna-7b-v1.3\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the laws of thermodynamics in simple terms.\"\n",
    "\"What are the key differences between mitosis and meiosis?\"\n",
    "\"How does machine learning improve image recognition?\"\n",
    "\"Describe the process of photosynthesis step by step.\"\n",
    "\"What are the applications of calculus in physics?\"\n",
    "\"Explain the significance of the Pythagorean theorem in geometry.\"\n",
    "\"How do electric circuits work in basic electronic devices?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 34 tokens in 49.34 seconds.\n",
      "Tokens per second: 0.69\n"
     ]
    }
   ],
   "source": [
    "prompt=\"Write a function that takes an array of integers and returns the largest number.\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 37 tokens in 53.46 seconds.\n",
      "Tokens per second: 0.69\n"
     ]
    }
   ],
   "source": [
    "prompt=\"Write a function that checks if a given number is prime.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 15 tokens in 22.89 seconds.\n",
      "Tokens per second: 0.66\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a function that takes a string as input and returns the reversed string.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 37 tokens in 55.48 seconds.\n",
      "Tokens per second: 0.67\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a function that checks if a number is even or odd.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 40 tokens in 60.63 seconds.\n",
      "Tokens per second: 0.66\n"
     ]
    }
   ],
   "source": [
    "prompt=\"What are the applications of calculus in physics?.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 tokens in 3.01 seconds.\n",
      "Tokens per second: 0.66\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the significance of the Pythagorean theorem in geometry..\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 36 tokens in 55.06 seconds.\n",
      "Tokens per second: 0.65\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How do electric circuits work in basic electronic devices?\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 39 tokens in 57.53 seconds.\n",
      "Tokens per second: 0.68\n"
     ]
    }
   ],
   "source": [
    "prompt =\"From the provided article, extract all factual statements about the topic of renewable energy: 'Solar and wind energy are becoming increasingly popular due to their sustainability and lower environmental impact.'\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
