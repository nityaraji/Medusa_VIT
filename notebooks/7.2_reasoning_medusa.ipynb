{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from medusa.model.modeling_llama_kv import LlamaForCausalLM as KVLlamaForCausalLM\n",
    "from medusa.model.medusa_model import MedusaModel, MedusaConfig\n",
    "from medusa.model.kv_cache import *\n",
    "from medusa.model.utils import *\n",
    "from medusa.model.medusa_choices import *\n",
    "import transformers\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timed(wall_times, key):\n",
    "    start = time.time()\n",
    "    torch.cuda.synchronize()\n",
    "    yield\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    wall_times[key].append(elapsed_time)\n",
    "\n",
    "def medusa_forward(input_ids, model, tokenizer, medusa_choices, temperature, posterior_threshold, posterior_alpha, max_steps = 512):\n",
    "    wall_times = {'medusa': [], 'tree': [], 'posterior': [], 'update': [], 'init': []}\n",
    "    \n",
    "    with timed(wall_times, 'init'):\n",
    "        if hasattr(model, \"medusa_choices\") and model.medusa_choices == medusa_choices:\n",
    "            medusa_buffers = model.medusa_buffers\n",
    "        else:\n",
    "            medusa_buffers = generate_medusa_buffers(\n",
    "                medusa_choices, device=model.base_model.device\n",
    "            )\n",
    "        model.medusa_buffers = medusa_buffers\n",
    "        model.medusa_choices = medusa_choices\n",
    "\n",
    "        if hasattr(model, \"past_key_values\"):\n",
    "            past_key_values = model.past_key_values\n",
    "            past_key_values_data = model.past_key_values_data\n",
    "            current_length_data = model.current_length_data\n",
    "            current_length_data.zero_()\n",
    "        else:\n",
    "            (\n",
    "                past_key_values,\n",
    "                past_key_values_data,\n",
    "                current_length_data,\n",
    "            ) = initialize_past_key_values(model.base_model)\n",
    "            model.past_key_values = past_key_values\n",
    "            model.past_key_values_data = past_key_values_data\n",
    "            model.current_length_data = current_length_data\n",
    "\n",
    "        input_len = input_ids.shape[1]\n",
    "        reset_medusa_mode(model)\n",
    "        medusa_logits, logits = initialize_medusa(\n",
    "                input_ids, model, medusa_buffers[\"medusa_attn_mask\"], past_key_values\n",
    "        )\n",
    "    new_token = 0\n",
    "\n",
    "    for idx in range(max_steps): \n",
    "        with timed(wall_times, 'medusa'):\n",
    "            candidates, tree_candidates = generate_candidates(\n",
    "                    medusa_logits,\n",
    "                    logits,\n",
    "                    medusa_buffers[\"tree_indices\"],\n",
    "                    medusa_buffers[\"retrieve_indices\"],\n",
    "                )\n",
    "\n",
    "        with timed(wall_times, 'tree'):\n",
    "            medusa_logits, logits, outputs = tree_decoding(\n",
    "                    model,\n",
    "                    tree_candidates,\n",
    "                    past_key_values,\n",
    "                    medusa_buffers[\"medusa_position_ids\"],\n",
    "                    input_ids,\n",
    "                    medusa_buffers[\"retrieve_indices\"],\n",
    "                )\n",
    "\n",
    "        with timed(wall_times, 'posterior'):\n",
    "            best_candidate, accept_length = evaluate_posterior(\n",
    "                    logits, candidates, temperature, posterior_threshold, posterior_alpha\n",
    "                )\n",
    "        \n",
    "        with timed(wall_times, 'update'):\n",
    "            input_ids, logits, medusa_logits, new_token = update_inference_inputs(\n",
    "                    input_ids,\n",
    "                    candidates,\n",
    "                    best_candidate,\n",
    "                    accept_length,\n",
    "                    medusa_buffers[\"retrieve_indices\"],\n",
    "                    outputs,\n",
    "                    logits,\n",
    "                    medusa_logits,\n",
    "                    new_token,\n",
    "                    past_key_values_data,\n",
    "                    current_length_data,\n",
    "                )\n",
    "\n",
    "        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():\n",
    "            break\n",
    "\n",
    "    return input_ids, new_token, idx, wall_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it]\n",
      "Some weights of MedusaModelLlama were not initialized from the model checkpoint at lmsys/vicuna-7b-v1.3 and are newly initialized: ['medusa_head.3.0.linear.bias', 'medusa_head.4.0.linear.weight', 'medusa_head.1.0.linear.weight', 'medusa_head.4.0.linear.bias', 'medusa_head.0.1.weight', 'medusa_head.0.0.linear.bias', 'medusa_head.1.1.weight', 'medusa_head.0.0.linear.weight', 'medusa_head.3.0.linear.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.2.0.linear.weight', 'medusa_head.2.1.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.4.1.weight', 'medusa_head.3.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "C:\\Users\\admin\\Downloads\\Medusa-main\\medusa\\model\\medusa_model.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  medusa_head_state_dict = torch.load(filename, map_location=model.device)\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 0.0.linear.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 0.0.linear.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 0.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 1.0.linear.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 1.0.linear.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 1.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 2.0.linear.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 2.0.linear.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 2.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 3.0.linear.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 3.0.linear.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 3.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 4.0.linear.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 4.0.linear.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2068: UserWarning: for 4.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"
     ]
    }
   ],
   "source": [
    "model_name = 'FasterDecoding/medusa-vicuna-7b-v1.3'\n",
    "\n",
    "config = MedusaConfig.from_pretrained(\n",
    "    model_name,\n",
    "    medusa_num_heads=4,\n",
    "    medusa_num_layers=1,\n",
    ")\n",
    "\n",
    "model = MedusaModel.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "medusa_choices = mc_sim_7b_63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.\n",
    "posterior_threshold = 0.09\n",
    "posterior_alpha = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Statement: 'We should hire more employees to reduce the workload on our current staff.'What assumption is being made in the above statement?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 178\n",
      "Compression ratio: tensor(1.0056, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A) The current staff is overworked\n",
      "B) The workload can be reduced by hiring more employees\n",
      "C) The workload can be reduced by increasing the efficiency of the current staff\n",
      "D) The workload cannot be reduced by hiring more employees\n",
      "\n",
      "Answer: B) The workload can be reduced by hiring more employees\n",
      "\n",
      "Explanation: The statement assumes that by hiring more employees, the workload on the current staff can be reduced. This assumption is based on the belief that the additional employees can share the workload and help reduce the burden on the current staff. However, it is important to consider other factors such as the workload distribution, employee skills, and the overall workload management system before making a decision to hire more employees.\n",
      "\n",
      "Bloom's Taxonomy Level: Comprehension</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              1.568\n",
      "Wall time medusa:                            0.106\n",
      "Wall time Tree:                            110.902\n",
      "Wall time Posterior:                         0.095\n",
      "Wall time Update:                            0.148\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.983\n",
      "Wall time portion Posterior:                 0.001\n",
      "Wall time portion Update:                    0.001\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.578\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"If all apples are fruits, and some fruits are sweet, what can you conclude about apples?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 110\n",
      "Compression ratio: tensor(1.0092, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If all apples are fruits, and some fruits are sweet, then we can conclude that some apples are sweet.\n",
      "If all apples are fruits, and some fruits are sweet, then we can conclude that some apples are sweet.\n",
      "If all apples are fruits, and some fruits are sweet, then we can conclude that all apples are sweet.\n",
      "If all apples are fruits, and some fruits are sweet, then we can conclude that some apples are not sweet.</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.930\n",
      "Wall time medusa:                            0.048\n",
      "Wall time Tree:                             68.556\n",
      "Wall time Posterior:                         0.016\n",
      "Wall time Update:                            0.099\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.984\n",
      "Wall time portion Posterior:                 0.000\n",
      "Wall time portion Update:                    0.001\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.579\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \" Passage: Sarah ordered a large pizza and ate only two slices. She placed the rest in the fridge to have later. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 57\n",
      "Compression ratio: tensor(1.0179, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question: What did Sarah do with the rest of the pizza?\n",
      "\n",
      "A) She threw it away.\n",
      "B) She gave it to her friend.\n",
      "C) She put it in the freezer.\n",
      "D) She left it on the counter.</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.970\n",
      "Wall time medusa:                            0.025\n",
      "Wall time Tree:                             35.693\n",
      "Wall time Posterior:                         0.021\n",
      "Wall time Update:                            0.042\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.971\n",
      "Wall time portion Posterior:                 0.001\n",
      "Wall time portion Update:                    0.001\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.551\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Statement:The roads are wet, and people are carrying umbrellas.What can you infer from this statement?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 512\n",
      "Compression ratio: tensor(1.0020, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. The weather is rainy.B. People are going to a concert.C. People are going to a park.D. People are going to a party.\n",
      "\n",
      "Solution: A. The weather is rainy.\n",
      "\n",
      "Explanation: The statement mentions that the roads are wet and people are carrying umbrellas, which suggests that it is raining outside. Therefore, the most likely inference is that the weather is rainy.\n",
      "\n",
      "Bloom'<s> The first time I saw him, I was walking down the street, and he was sitting on a bench. He was wearing a suit and tie, and he had a newspaper in his hand. He looked up and saw me, and he smiled. I was taken aback by his smile. It was the kind of smile that could light up a room, and it made me feel like I was the only person in the world.\n",
      "\n",
      "I walked over to him, and he stood up. He introduced himself as Jack, and I introduced myself as Rachel. We talked for a few minutes, and I could tell that he was a kind and gentle man. He had a way of making me feel comfortable, and I felt like I could talk to him about anything.\n",
      "\n",
      "As we talked, I noticed that he was looking at me in a way that made me feel special. He was listening to me, and he was making eye contact. He was showing me that he was interested in me, and I felt a spark of attraction.\n",
      "\n",
      "After we had talked for a while, he asked me if I would like to go out with him. I was surprised, but I said yes. We went out a few times, and I could tell that he was a good man. He was kind and thoughtful, and he made me feel special.\n",
      "\n",
      "One day, he took me to a park, and we sat on a bench. He took my hand, and he looked into my eyes. He told me that he loved me, and I felt my heart swell with happiness. I knew that I loved him too, and I felt like the luckiest person in the world.\n",
      "\n",
      "We got married a few months later, and we have been together ever since. We have had our ups and downs, but we have always loved each other. We have built a life together, and we have created a family. We have been through a lot, but we have always been there for\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              1.010\n",
      "Wall time medusa:                            0.279\n",
      "Wall time Tree:                            320.738\n",
      "Wall time Posterior:                         0.233\n",
      "Wall time Update:                            0.356\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.994\n",
      "Wall time portion Posterior:                 0.001\n",
      "Wall time portion Update:                    0.001\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.587\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Complete the analogy:'Book is to Author as Painting is to________.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 29\n",
      "Compression ratio: tensor(1.0357, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "a) Artist\n",
      "b) Artwork\n",
      "c) Gallery\n",
      "d) Critic\n",
      "\n",
      "Answer: b) Artwork</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.845\n",
      "Wall time medusa:                            0.013\n",
      "Wall time Tree:                             18.098\n",
      "Wall time Posterior:                         0.014\n",
      "Wall time Update:                            0.023\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.953\n",
      "Wall time portion Posterior:                 0.001\n",
      "Wall time portion Update:                    0.001\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.527\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Statement: The restaurant is full, and there's a waiting time of 30 minutes.What can you infer about the restaurant's popularity?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 232\n",
      "Compression ratio: tensor(1.0043, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A. It is a new restaurant that has not yet gained a reputation.\n",
      "B. It is a restaurant that serves a unique cuisine that is not widely available.\n",
      "C. It is a restaurant that is located in a remote area.\n",
      "D. It is a restaurant that is not well-known or popular.\n",
      "E. It is a restaurant that is located in a busy city center.\n",
      "\n",
      "Answer: E. It is a restaurant that is located in a busy city center.\n",
      "\n",
      "Explanation: The fact that the restaurant is full and there is a waiting time of 30 minutes indicates that it is a popular restaurant located in a busy city center. This suggests that it is a well-known restaurant that serves a widely available cuisine, and therefore, it is not a new restaurant that has not yet gained a reputation. It is also possible that the restaurant is located in a remote area, but this is less likely given that it has a waiting time of 30 minutes. Therefore, the most reasonable inference is that the restaurant is located in a busy city center and is popular among customers.</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.862\n",
      "Wall time medusa:                            0.118\n",
      "Wall time Tree:                            145.553\n",
      "Wall time Posterior:                         0.097\n",
      "Wall time Update:                            0.175\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.991\n",
      "Wall time portion Posterior:                 0.001\n",
      "Wall time portion Update:                    0.001\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.580\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"Arrange the following events in the correct order: 1)Eat dinner 2)Prepare ingredients 3)Cook the meal 4)Set the table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output length: 53\n",
      "Compression ratio: tensor(1.0192, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5)Wash the dishes\n",
      "\n",
      "The correct order is: 1)Prepare ingredients 2)Cook the meal 3)Set the table 4)Eat dinner 5)Wash the dishes.</s>\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Wall time init:                              0.859\n",
      "Wall time medusa:                            0.027\n",
      "Wall time Tree:                             34.847\n",
      "Wall time Posterior:                         0.021\n",
      "Wall time Update:                            0.037\n",
      "--------------------------------------------------\n",
      "Wall time portion medusa:                    0.001\n",
      "Wall time portion Tree:                      0.974\n",
      "Wall time portion Posterior:                 0.001\n",
      "Wall time portion Update:                    0.001\n",
      "--------------------------------------------------\n",
      "Tokens/second:                               1.481\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
