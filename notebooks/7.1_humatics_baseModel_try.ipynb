{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # define GPU id, remove if you want to use all GPUs available\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import transformers\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"lmsys/vicuna-7b-v1.3\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.04s/it]\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 39, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 tokens in 1.17 seconds.\n",
      "Tokens per second: 0.85\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"lmsys/vicuna-7b-v1.3\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "prompt = \"How does Immanuel Kant’s categorical imperative differ from John Stuart Mill’s utilitarianism when applied to modern-day ethical dilemmas like climate change?\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 32509824 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 37 tokens in 59.69 seconds.\n",
      "Tokens per second: 0.62\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"lmsys/vicuna-7b-v1.3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\", \n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "prompt = \"How does Immanuel Kant’s categorical imperative differ from John Stuart Mill’s utilitarianism when applied to modern-day ethical dilemmas like climate change?\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)\n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 tokens in 4.63 seconds.\n",
      "Tokens per second: 0.65\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Analyze the theme of existentialism in Fyodor Dostoevsky’s 'Notes from Underground' and Albert Camus’ 'The Stranger.' How do the protagonists confront the absurd?\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 33, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 tokens in 1.60 seconds.\n",
      "Tokens per second: 0.63\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How did the Italian Renaissance contribute to the evolution of individualism in European art, and what parallels can be drawn to modern art movements like Expressionism?\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 32, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 18 tokens in 29.04 seconds.\n",
      "Tokens per second: 0.62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"In what ways has globalization transformed indigenous cultural practices? Discuss the tension between cultural preservation and modernization in today’s world.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 41, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9 tokens in 13.53 seconds.\n",
      "Tokens per second: 0.67\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explore how Virginia Woolf’s concept of ‘a room of one’s own’ has shaped modern feminist thought and its influence on contemporary discussions of gender and creativity\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 42, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8 tokens in 12.16 seconds.\n",
      "Tokens per second: 0.66\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Compare and contrast the views of Karl Marx and Max Weber on the role of the state in shaping economic inequality. How relevant are their ideas in analyzing today’s capitalist societies?\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Medusa-main\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 45, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 tokens in 7.73 seconds.\n",
      "Tokens per second: 0.65\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Examine how the concept of the ‘Hero’s Journey,’ as outlined by Joseph Campbell, is reflected in various religious narratives. What common themes emerge across different world religions?\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "_ = model.generate(input_ids, max_length=20)\n",
    "\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_generated_tokens = output.size(-1) - input_ids.size(-1)  \n",
    "elapsed_time = end_time - start_time\n",
    "tokens_per_second = num_generated_tokens / elapsed_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
